import{_ as t,c as o,o as i,aM as s}from"./chunks/framework.IHAHILrl.js";const h=JSON.parse('{"title":"OpenAI","description":"","frontmatter":{"title":"OpenAI","date":"2025-09-02T01:16:27.000Z"},"headers":[],"relativePath":"docs/providers/openai.md","filePath":"docs/providers/openai.md","lastUpdated":1763178133000}'),r={name:"docs/providers/openai.md"};function a(n,e,l,p,g,d){return i(),o("div",null,[...e[0]||(e[0]=[s(`<h1 id="openai-provider" tabindex="-1">OpenAI Provider <a class="header-anchor" href="#openai-provider" aria-label="Permalink to &quot;OpenAI Provider&quot;">‚Äã</a></h1><p>OpenAI offers industry-leading AI models including GPT-4, GPT-3.5, and the reasoning-focused o1 series.</p><h2 id="getting-started" tabindex="-1">Getting Started <a class="header-anchor" href="#getting-started" aria-label="Permalink to &quot;Getting Started&quot;">‚Äã</a></h2><h3 id="create-an-api-key" tabindex="-1">Create an API Key <a class="header-anchor" href="#create-an-api-key" aria-label="Permalink to &quot;Create an API Key&quot;">‚Äã</a></h3><ol><li>Visit <a href="https://platform.openai.com" target="_blank" rel="noreferrer">https://platform.openai.com</a></li><li>Sign up or log in</li><li>Navigate to <strong>API keys</strong> section</li><li>Click <strong>Create new secret key</strong></li><li><strong>Copy the key immediately</strong> (it won&#39;t be shown again)</li><li><strong>Name it</strong> for tracking (e.g., &quot;Kelivo Mobile&quot;)</li></ol><h3 id="add-to-kelivo" tabindex="-1">Add to Kelivo <a class="header-anchor" href="#add-to-kelivo" aria-label="Permalink to &quot;Add to Kelivo&quot;">‚Äã</a></h3><ol><li>Go to <strong>Settings ‚Üí Providers</strong></li><li>Tap <strong>Add Provider</strong> or <strong>+</strong></li><li>Fill in details: <ul><li><strong>Name</strong>: &quot;OpenAI&quot; or custom name</li><li><strong>Provider Type</strong>: OpenAI</li><li><strong>Base URL</strong>: <code>https://api.openai.com/v1</code> (default)</li><li><strong>API Key</strong>: Paste your key</li><li><strong>Default Model</strong>: gpt-4o (or your preferred model)</li></ul></li><li><strong>Save</strong></li></ol><h3 id="test-connection" tabindex="-1">Test Connection <a class="header-anchor" href="#test-connection" aria-label="Permalink to &quot;Test Connection&quot;">‚Äã</a></h3><p>Send a test message to verify:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>User: Hello!</span></span>
<span class="line"><span>AI: Hello! How can I help you today?</span></span></code></pre></div><p>If it works, you&#39;re all set!</p><h2 id="available-models" tabindex="-1">Available Models <a class="header-anchor" href="#available-models" aria-label="Permalink to &quot;Available Models&quot;">‚Äã</a></h2><h3 id="gpt-4-series-most-capable" tabindex="-1">GPT-4 Series (Most Capable) <a class="header-anchor" href="#gpt-4-series-most-capable" aria-label="Permalink to &quot;GPT-4 Series (Most Capable)&quot;">‚Äã</a></h3><p><strong>gpt-4o</strong> (Recommended)</p><ul><li><strong>Best for</strong>: General use, vision, most tasks</li><li><strong>Context</strong>: 128K tokens</li><li><strong>Features</strong>: Text, vision, function calling, JSON mode</li><li><strong>Speed</strong>: Fast</li><li><strong>Cost</strong>: Moderate</li><li><strong>Use when</strong>: You need high quality and speed</li></ul><p><strong>gpt-4-turbo</strong></p><ul><li><strong>Best for</strong>: Complex tasks, long documents</li><li><strong>Context</strong>: 128K tokens</li><li><strong>Features</strong>: Text, vision, function calling</li><li><strong>Speed</strong>: Moderate</li><li><strong>Cost</strong>: Higher</li><li><strong>Use when</strong>: You need maximum capability</li></ul><p><strong>gpt-4</strong> (Original)</p><ul><li><strong>Best for</strong>: Consistency, proven reliability</li><li><strong>Context</strong>: 8K tokens</li><li><strong>Features</strong>: Text only</li><li><strong>Speed</strong>: Slower</li><li><strong>Cost</strong>: High</li><li><strong>Use when</strong>: Legacy applications</li></ul><h3 id="gpt-3-5-series-fast-economical" tabindex="-1">GPT-3.5 Series (Fast &amp; Economical) <a class="header-anchor" href="#gpt-3-5-series-fast-economical" aria-label="Permalink to &quot;GPT-3.5 Series (Fast &amp; Economical)&quot;">‚Äã</a></h3><p><strong>gpt-3.5-turbo</strong></p><ul><li><strong>Best for</strong>: Quick tasks, high volume, cost-sensitive use</li><li><strong>Context</strong>: 16K tokens</li><li><strong>Features</strong>: Text, function calling</li><li><strong>Speed</strong>: Very fast</li><li><strong>Cost</strong>: Low</li><li><strong>Use when</strong>: Simple tasks, speed matters</li></ul><h3 id="o1-series-reasoning-models" tabindex="-1">o1 Series (Reasoning Models) <a class="header-anchor" href="#o1-series-reasoning-models" aria-label="Permalink to &quot;o1 Series (Reasoning Models)&quot;">‚Äã</a></h3><p><strong>o1-preview</strong></p><ul><li><strong>Best for</strong>: Complex reasoning, math, code, science</li><li><strong>Context</strong>: 128K tokens</li><li><strong>Features</strong>: Deep reasoning, no streaming</li><li><strong>Speed</strong>: Slower (thinks before responding)</li><li><strong>Cost</strong>: Higher</li><li><strong>Use when</strong>: Problems require step-by-step thinking</li></ul><p><strong>o1-mini</strong></p><ul><li><strong>Best for</strong>: Coding, STEM problems, cost-conscious reasoning</li><li><strong>Context</strong>: 128K tokens</li><li><strong>Features</strong>: Faster reasoning, more affordable</li><li><strong>Speed</strong>: Moderate</li><li><strong>Cost</strong>: Lower than o1-preview</li><li><strong>Use when</strong>: Coding or math on a budget</li></ul><h3 id="model-comparison" tabindex="-1">Model Comparison <a class="header-anchor" href="#model-comparison" aria-label="Permalink to &quot;Model Comparison&quot;">‚Äã</a></h3><table tabindex="0"><thead><tr><th>Model</th><th>Speed</th><th>Quality</th><th>Cost</th><th>Vision</th><th>Best Use Case</th></tr></thead><tbody><tr><td>gpt-4o</td><td>‚ö°‚ö°‚ö°</td><td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td><td>üí∞üí∞</td><td>‚úÖ</td><td>General purpose</td></tr><tr><td>gpt-4-turbo</td><td>‚ö°‚ö°</td><td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td><td>üí∞üí∞üí∞</td><td>‚úÖ</td><td>Long documents</td></tr><tr><td>gpt-3.5-turbo</td><td>‚ö°‚ö°‚ö°‚ö°</td><td>‚≠ê‚≠ê‚≠ê</td><td>üí∞</td><td>‚ùå</td><td>Simple tasks</td></tr><tr><td>o1-preview</td><td>‚ö°</td><td>‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td><td>üí∞üí∞üí∞üí∞</td><td>‚ùå</td><td>Complex reasoning</td></tr><tr><td>o1-mini</td><td>‚ö°‚ö°</td><td>‚≠ê‚≠ê‚≠ê‚≠ê</td><td>üí∞üí∞</td><td>‚ùå</td><td>Coding, STEM</td></tr></tbody></table><h2 id="features-capabilities" tabindex="-1">Features &amp; Capabilities <a class="header-anchor" href="#features-capabilities" aria-label="Permalink to &quot;Features &amp; Capabilities&quot;">‚Äã</a></h2><h3 id="vision-multimodal" tabindex="-1">Vision (Multimodal) <a class="header-anchor" href="#vision-multimodal" aria-label="Permalink to &quot;Vision (Multimodal)&quot;">‚Äã</a></h3><p><strong>Supported models</strong>: gpt-4o, gpt-4-turbo, gpt-4-vision-preview</p><p><strong>Use cases</strong>:</p><ul><li>Describe images</li><li>Extract text from screenshots</li><li>Analyze charts and diagrams</li><li>Answer questions about photos</li><li>Generate image captions</li></ul><p><strong>Example</strong>:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>User: [attaches screenshot] What&#39;s in this error message?</span></span>
<span class="line"><span>AI: The error shows a &quot;Module not found&quot; exception...</span></span></code></pre></div><p><strong>Tips</strong>:</p><ul><li>High-resolution images cost more tokens</li><li>Consider resizing large images</li><li>Works with multiple images in one message</li></ul><h3 id="function-calling" tabindex="-1">Function Calling <a class="header-anchor" href="#function-calling" aria-label="Permalink to &quot;Function Calling&quot;">‚Äã</a></h3><p><strong>Supported models</strong>: All except o1 series (limited support)</p><p><strong>Use cases</strong>:</p><ul><li>Structured data extraction</li><li>API integrations</li><li>Tool usage (web search, calculations)</li><li>Database queries</li></ul><p><strong>Example</strong>:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>User: What&#39;s the weather in Tokyo?</span></span>
<span class="line"><span>AI: [calls get_weather function with location=&quot;Tokyo&quot;]</span></span>
<span class="line"><span>AI: It&#39;s currently 22¬∞C and partly cloudy in Tokyo.</span></span></code></pre></div><h3 id="json-mode" tabindex="-1">JSON Mode <a class="header-anchor" href="#json-mode" aria-label="Permalink to &quot;JSON Mode&quot;">‚Äã</a></h3><p><strong>Supported models</strong>: gpt-4o, gpt-4-turbo, gpt-3.5-turbo</p><p><strong>Use cases</strong>:</p><ul><li>Structured output</li><li>Data extraction</li><li>API responses</li></ul><p><strong>Enable</strong>: Add to request (via custom prompts or provider settings)</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>{</span></span>
<span class="line"><span>  &quot;response_format&quot;: { &quot;type&quot;: &quot;json_object&quot; }</span></span>
<span class="line"><span>}</span></span></code></pre></div><p><strong>Example</strong>:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>User: Extract: &quot;John works at Google as a developer&quot;</span></span>
<span class="line"><span>AI: {&quot;name&quot;: &quot;John&quot;, &quot;company&quot;: &quot;Google&quot;, &quot;role&quot;: &quot;developer&quot;}</span></span></code></pre></div><h3 id="streaming" tabindex="-1">Streaming <a class="header-anchor" href="#streaming" aria-label="Permalink to &quot;Streaming&quot;">‚Äã</a></h3><p><strong>All models support streaming</strong> for real-time responses.</p><p><strong>Benefits</strong>:</p><ul><li>Faster perceived response time</li><li>Can stop generation mid-response</li><li>Better user experience</li></ul><p>Kelivo enables streaming by default.</p><h2 id="configuration-tips" tabindex="-1">Configuration Tips <a class="header-anchor" href="#configuration-tips" aria-label="Permalink to &quot;Configuration Tips&quot;">‚Äã</a></h2><h3 id="temperature-settings" tabindex="-1">Temperature Settings <a class="header-anchor" href="#temperature-settings" aria-label="Permalink to &quot;Temperature Settings&quot;">‚Äã</a></h3><p><strong>For different models</strong>:</p><ul><li><strong>GPT-4o, GPT-4</strong>: 0.7 (default, balanced)</li><li><strong>GPT-3.5</strong>: 0.5-0.7 (more focused)</li><li><strong>o1 series</strong>: Fixed internally (don&#39;t adjust)</li></ul><p><strong>For different tasks</strong>:</p><ul><li><strong>Code</strong>: 0.2-0.3 (deterministic)</li><li><strong>Writing</strong>: 0.8-1.0 (creative)</li><li><strong>Analysis</strong>: 0.3-0.5 (focused)</li><li><strong>Brainstorming</strong>: 1.0-1.2 (diverse)</li></ul><h3 id="max-tokens" tabindex="-1">Max Tokens <a class="header-anchor" href="#max-tokens" aria-label="Permalink to &quot;Max Tokens&quot;">‚Äã</a></h3><p>Control response length:</p><ul><li><strong>Short answers</strong>: 256-512 tokens</li><li><strong>Medium</strong>: 1024-2048 tokens</li><li><strong>Long form</strong>: 4096+ tokens</li></ul><p><strong>Tip</strong>: Leave headroom in context window for conversation history.</p><h3 id="system-prompts" tabindex="-1">System Prompts <a class="header-anchor" href="#system-prompts" aria-label="Permalink to &quot;System Prompts&quot;">‚Äã</a></h3><p>OpenAI models respond well to:</p><ul><li>Clear role definitions</li><li>Numbered instructions</li><li>Examples (few-shot)</li><li>Explicit formatting requests</li></ul><p><strong>Example</strong>:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>You are a senior Python developer. Provide clean, well-documented code with type hints. Always consider edge cases and error handling.</span></span></code></pre></div><h2 id="pricing-rate-limits" tabindex="-1">Pricing &amp; Rate Limits <a class="header-anchor" href="#pricing-rate-limits" aria-label="Permalink to &quot;Pricing &amp; Rate Limits&quot;">‚Äã</a></h2><h3 id="pricing-as-of-2025" tabindex="-1">Pricing (as of 2025) <a class="header-anchor" href="#pricing-as-of-2025" aria-label="Permalink to &quot;Pricing (as of 2025)&quot;">‚Äã</a></h3><p><strong>Input/Output tokens</strong>:</p><ul><li><strong>gpt-4o</strong>: $2.50/$10.00 per 1M tokens</li><li><strong>gpt-4-turbo</strong>: $10.00/$30.00 per 1M tokens</li><li><strong>gpt-3.5-turbo</strong>: $0.50/$1.50 per 1M tokens</li><li><strong>o1-preview</strong>: $15.00/$60.00 per 1M tokens</li><li><strong>o1-mini</strong>: $3.00/$12.00 per 1M tokens</li></ul><p><strong>Note</strong>: Prices may change. Check <a href="https://openai.com/pricing" target="_blank" rel="noreferrer">https://openai.com/pricing</a> for current rates.</p><h3 id="rate-limits" tabindex="-1">Rate Limits <a class="header-anchor" href="#rate-limits" aria-label="Permalink to &quot;Rate Limits&quot;">‚Äã</a></h3><p><strong>Free tier</strong>:</p><ul><li>3 requests per minute (RPM)</li><li>40,000 tokens per minute (TPM)</li><li>200 requests per day (RPD)</li></ul><p><strong>Paid tier 1</strong>:</p><ul><li>3,500 RPM (GPT-4)</li><li>90,000 TPM</li><li>Higher limits after usage</li></ul><p><strong>Paid tier 2+</strong>:</p><ul><li>5,000+ RPM</li><li>450,000+ TPM</li><li>Scale with usage</li></ul><p><strong>Tip</strong>: Upgrade to paid tier for better limits. Monitor usage in OpenAI dashboard.</p><h2 id="troubleshooting" tabindex="-1">Troubleshooting <a class="header-anchor" href="#troubleshooting" aria-label="Permalink to &quot;Troubleshooting&quot;">‚Äã</a></h2><h3 id="rate-limit-errors-429" tabindex="-1">Rate Limit Errors (429) <a class="header-anchor" href="#rate-limit-errors-429" aria-label="Permalink to &quot;Rate Limit Errors (429)&quot;">‚Äã</a></h3><p><strong>Error</strong>: &quot;Rate limit exceeded&quot;</p><p><strong>Solutions</strong>:</p><ol><li><strong>Wait</strong>: Limits reset per minute</li><li><strong>Upgrade tier</strong>: Use more, get higher limits</li><li><strong>Switch model</strong>: Use GPT-3.5 if GPT-4 is limited</li><li><strong>Reduce frequency</strong>: Add delays between requests</li></ol><h3 id="invalid-api-key-401" tabindex="-1">Invalid API Key (401) <a class="header-anchor" href="#invalid-api-key-401" aria-label="Permalink to &quot;Invalid API Key (401)&quot;">‚Äã</a></h3><p><strong>Error</strong>: &quot;Incorrect API key provided&quot;</p><p><strong>Solutions</strong>:</p><ol><li><strong>Verify key</strong>: Copy entire key, no extra spaces</li><li><strong>Check status</strong>: Ensure key hasn&#39;t been revoked</li><li><strong>Regenerate</strong>: Create new key if lost</li><li><strong>Organization</strong>: Verify org access if applicable</li></ol><h3 id="model-not-found-404" tabindex="-1">Model Not Found (404) <a class="header-anchor" href="#model-not-found-404" aria-label="Permalink to &quot;Model Not Found (404)&quot;">‚Äã</a></h3><p><strong>Error</strong>: &quot;The model &#39;xyz&#39; does not exist&quot;</p><p><strong>Solutions</strong>:</p><ol><li><strong>Check name</strong>: Use exact model names (e.g., &quot;gpt-4o&quot; not &quot;gpt4o&quot;)</li><li><strong>Access</strong>: Verify your account has access (o1 requires waitlist approval)</li><li><strong>Region</strong>: Some models limited to certain regions</li></ol><h3 id="context-length-exceeded" tabindex="-1">Context Length Exceeded <a class="header-anchor" href="#context-length-exceeded" aria-label="Permalink to &quot;Context Length Exceeded&quot;">‚Äã</a></h3><p><strong>Error</strong>: &quot;This model&#39;s maximum context length is...&quot;</p><p><strong>Solutions</strong>:</p><ol><li><strong>Shorten input</strong>: Reduce conversation history</li><li><strong>Use larger model</strong>: GPT-4-turbo has 128K context</li><li><strong>Summarize</strong>: Condense old messages</li><li><strong>Clear history</strong>: Start fresh conversation</li></ol><h3 id="slow-responses" tabindex="-1">Slow Responses <a class="header-anchor" href="#slow-responses" aria-label="Permalink to &quot;Slow Responses&quot;">‚Äã</a></h3><p><strong>Solutions</strong>:</p><ol><li><strong>Use faster model</strong>: GPT-3.5 or gpt-4o</li><li><strong>Streaming</strong>: Should be enabled by default</li><li><strong>Reduce load</strong>: Shorter prompts, less context</li><li><strong>Check status</strong>: <a href="https://status.openai.com" target="_blank" rel="noreferrer">https://status.openai.com</a></li></ol><h2 id="advanced-features" tabindex="-1">Advanced Features <a class="header-anchor" href="#advanced-features" aria-label="Permalink to &quot;Advanced Features&quot;">‚Äã</a></h2><h3 id="organization-id" tabindex="-1">Organization ID <a class="header-anchor" href="#organization-id" aria-label="Permalink to &quot;Organization ID&quot;">‚Äã</a></h3><p>For teams with multiple projects:</p><ol><li>Find your org ID: <a href="https://platform.openai.com/account/organization" target="_blank" rel="noreferrer">https://platform.openai.com/account/organization</a></li><li>Add custom header in Kelivo:<div class="language-json vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">json</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">{</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  &quot;OpenAI-Organization&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;org-xxxxxxxxxxxxx&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div></li></ol><h3 id="azure-openai" tabindex="-1">Azure OpenAI <a class="header-anchor" href="#azure-openai" aria-label="Permalink to &quot;Azure OpenAI&quot;">‚Äã</a></h3><p>Use OpenAI models via Azure:</p><p><strong>Base URL</strong>:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>https://YOUR-RESOURCE-NAME.openai.azure.com</span></span></code></pre></div><p><strong>API Key</strong>: Azure OpenAI key (not OpenAI key)</p><p><strong>Model Deployment</strong>: Use your deployment name</p><p>See <a href="/docs/assistant/custom-requests">Custom Requests</a> for details.</p><h3 id="openrouter-access-200-models-via-openai-api" tabindex="-1">OpenRouter - Access 200+ Models via OpenAI API <a class="header-anchor" href="#openrouter-access-200-models-via-openai-api" aria-label="Permalink to &quot;OpenRouter - Access 200+ Models via OpenAI API&quot;">‚Äã</a></h3><p><strong>OpenRouter</strong> is an OpenAI-compatible gateway that aggregates models from multiple providers into a single API. It offers significantly more model choices than OpenAI alone, with unified pricing and a single API key.</p><h4 id="why-use-openrouter" tabindex="-1">Why Use OpenRouter? <a class="header-anchor" href="#why-use-openrouter" aria-label="Permalink to &quot;Why Use OpenRouter?&quot;">‚Äã</a></h4><p><strong>Access to 200+ Models</strong>:</p><ul><li>OpenAI (GPT-4, GPT-3.5)</li><li>Anthropic (Claude 3.5, Claude 3)</li><li>Google (Gemini Pro, Gemini Flash)</li><li>Meta (Llama 3.1, Llama 3.2)</li><li>Mistral (Mistral Large, Mistral Medium)</li><li>Cohere (Command R+)</li><li>Open-source models (Mixtral, Yi, Qwen)</li><li>And many more</li></ul><p><strong>Benefits</strong>:</p><ul><li><strong>Single API key</strong> for all providers</li><li><strong>Unified pricing</strong> across models</li><li><strong>Lower costs</strong> for many models</li><li><strong>No rate limits</strong> on most models</li><li><strong>Fallback support</strong> (auto-switch if model is down)</li><li><strong>Model routing</strong> (automatically choose best model)</li><li><strong>Usage tracking</strong> in one dashboard</li></ul><h4 id="getting-started-with-openrouter" tabindex="-1">Getting Started with OpenRouter <a class="header-anchor" href="#getting-started-with-openrouter" aria-label="Permalink to &quot;Getting Started with OpenRouter&quot;">‚Äã</a></h4><p><strong>1. Create an API Key</strong></p><ol><li>Visit <a href="https://openrouter.ai" target="_blank" rel="noreferrer">https://openrouter.ai</a></li><li>Sign up or log in</li><li>Go to <strong>Keys</strong> section</li><li>Click <strong>Create Key</strong></li><li><strong>Name your key</strong> (e.g., &quot;Kelivo&quot;)</li><li><strong>Copy the key</strong> (starts with <code>sk-or-v1-</code>)</li><li><strong>Add credits</strong> (minimum $5, pay-as-you-go)</li></ol><p><strong>2. Add to Kelivo</strong></p><ol><li>Go to <strong>Settings ‚Üí Providers</strong></li><li>Tap <strong>Add Provider</strong> or <strong>+</strong></li><li>Fill in details: <ul><li><strong>Name</strong>: &quot;OpenRouter&quot; or custom name</li><li><strong>Provider Type</strong>: OpenAI (OpenRouter is OpenAI-compatible)</li><li><strong>Base URL</strong>: <code>https://openrouter.ai/api/v1</code></li><li><strong>API Key</strong>: Paste your OpenRouter key (sk-or-v1-...)</li><li><strong>Default Model</strong>: See model recommendations below</li></ul></li><li><strong>Save</strong></li></ol><p><strong>3. Test Connection</strong></p><p>Send a test message to verify:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>User: Hello!</span></span>
<span class="line"><span>AI: Hello! How can I help you today?</span></span></code></pre></div><h4 id="recommended-models-on-openrouter" tabindex="-1">Recommended Models on OpenRouter <a class="header-anchor" href="#recommended-models-on-openrouter" aria-label="Permalink to &quot;Recommended Models on OpenRouter&quot;">‚Äã</a></h4><p><strong>Best Value Models</strong>:</p><p><strong>anthropic/claude-3.5-sonnet</strong> (Highly Recommended)</p><ul><li><strong>Best for</strong>: General purpose, excellent quality</li><li><strong>Context</strong>: 200K tokens</li><li><strong>Cost</strong>: ~$3/$15 per 1M tokens (input/output)</li><li><strong>Features</strong>: Vision, function calling, artifacts</li><li><strong>Why</strong>: Best quality-to-price ratio, beats GPT-4o</li></ul><p><strong>google/gemini-pro-1.5</strong> (Free Tier Available)</p><ul><li><strong>Best for</strong>: Long context, cost-conscious use</li><li><strong>Context</strong>: 2M tokens (!!)</li><li><strong>Cost</strong>: FREE up to limits, then $0.35/$1.05 per 1M tokens</li><li><strong>Features</strong>: Vision, massive context window</li><li><strong>Why</strong>: Incredible value, huge context</li></ul><p><strong>meta-llama/llama-3.1-70b-instruct</strong> (Open Source)</p><ul><li><strong>Best for</strong>: Privacy-conscious, open source preference</li><li><strong>Context</strong>: 128K tokens</li><li><strong>Cost</strong>: ~$0.50/$0.75 per 1M tokens</li><li><strong>Features</strong>: Strong reasoning, fast</li><li><strong>Why</strong>: Excellent open-source option</li></ul><p><strong>openai/gpt-4o</strong> (Latest OpenAI)</p><ul><li><strong>Best for</strong>: Tasks requiring latest GPT-4 capabilities</li><li><strong>Context</strong>: 128K tokens</li><li><strong>Cost</strong>: $2.50/$10 per 1M tokens (same as OpenAI)</li><li><strong>Features</strong>: Vision, function calling, fast</li><li><strong>Why</strong>: Latest OpenAI model, same price</li></ul><p><strong>Budget Options</strong>:</p><p><strong>mistralai/mistral-7b-instruct</strong> (~$0.07/$0.07 per 1M tokens)</p><ul><li>Fast, cheap, good for simple tasks</li></ul><p><strong>google/gemini-flash-1.5</strong> (~$0.075/$0.30 per 1M tokens)</p><ul><li>Very fast, excellent for high-volume simple tasks</li></ul><p><strong>meta-llama/llama-3.2-3b-instruct</strong> (~$0.06/$0.06 per 1M tokens)</p><ul><li>Extremely cheap, good for basic tasks</li></ul><p><strong>Premium Options</strong>:</p><p><strong>anthropic/claude-3-opus</strong> (~$15/$75 per 1M tokens)</p><ul><li>Most capable Claude model, best reasoning</li></ul><p><strong>openai/o1-preview</strong> (~$15/$60 per 1M tokens)</p><ul><li>Advanced reasoning, complex problem solving</li></ul><p><strong>google/gemini-pro-1.5-exp</strong> (~$0.35/$1.05 per 1M tokens)</p><ul><li>Experimental Gemini with latest features</li></ul><h4 id="model-categories" tabindex="-1">Model Categories <a class="header-anchor" href="#model-categories" aria-label="Permalink to &quot;Model Categories&quot;">‚Äã</a></h4><p><strong>By Use Case</strong>:</p><p><strong>General Purpose</strong>:</p><ul><li>anthropic/claude-3.5-sonnet (best overall)</li><li>openai/gpt-4o (latest GPT-4)</li><li>google/gemini-pro-1.5 (best value)</li></ul><p><strong>Coding</strong>:</p><ul><li>anthropic/claude-3.5-sonnet (excellent at code)</li><li>meta-llama/llama-3.1-405b-instruct (open source, powerful)</li><li>openai/gpt-4o (strong coding)</li></ul><p><strong>Long Context</strong>:</p><ul><li>google/gemini-pro-1.5 (2M tokens!)</li><li>anthropic/claude-3.5-sonnet (200K tokens)</li><li>openai/gpt-4o (128K tokens)</li></ul><p><strong>Fast &amp; Cheap</strong>:</p><ul><li>google/gemini-flash-1.5 (very fast)</li><li>mistralai/mistral-7b-instruct (cheap)</li><li>meta-llama/llama-3.2-3b-instruct (cheapest)</li></ul><p><strong>Reasoning &amp; Analysis</strong>:</p><ul><li>openai/o1-preview (advanced reasoning)</li><li>anthropic/claude-3-opus (deep thinking)</li><li>perplexity/llama-3.1-sonar-large (web search integrated)</li></ul><p><strong>Vision</strong>:</p><ul><li>anthropic/claude-3.5-sonnet (excellent vision)</li><li>openai/gpt-4o (strong vision)</li><li>google/gemini-pro-1.5 (good vision)</li></ul><h4 id="pricing-comparison" tabindex="-1">Pricing Comparison <a class="header-anchor" href="#pricing-comparison" aria-label="Permalink to &quot;Pricing Comparison&quot;">‚Äã</a></h4><p>OpenRouter often offers <strong>better pricing</strong> than direct providers:</p><table tabindex="0"><thead><tr><th>Model</th><th>OpenRouter</th><th>Direct Provider</th><th>Savings</th></tr></thead><tbody><tr><td>Claude 3.5 Sonnet</td><td>$3/$15 per 1M</td><td>$3/$15 per 1M</td><td>Same</td></tr><tr><td>GPT-4o</td><td>$2.50/$10 per 1M</td><td>$2.50/$10 per 1M</td><td>Same</td></tr><tr><td>Gemini Pro 1.5</td><td>Free tier!</td><td>Paid only</td><td>Better</td></tr><tr><td>Llama 3.1 70B</td><td>$0.50/$0.75 per 1M</td><td>Self-host</td><td>Much easier</td></tr><tr><td>Mistral 7B</td><td>$0.07/$0.07 per 1M</td><td>‚Ç¨0.20+</td><td>~65% savings</td></tr></tbody></table><p><strong>Pay-as-you-go</strong>: No subscriptions, only pay for what you use.</p><h4 id="openrouter-specific-features" tabindex="-1">OpenRouter-Specific Features <a class="header-anchor" href="#openrouter-specific-features" aria-label="Permalink to &quot;OpenRouter-Specific Features&quot;">‚Äã</a></h4><p><strong>Model Fallbacks</strong>: Add to custom headers:</p><div class="language-json vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">json</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">{</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  &quot;X-Title&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Kelivo Chat&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  &quot;HTTP-Referer&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;https://kelivo.app&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  &quot;X-Model-Fallbacks&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;anthropic/claude-3.5-sonnet&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;openai/gpt-4o&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div><p>If primary model fails, automatically uses fallback.</p><p><strong>Model Routing</strong> (Auto-select): Use <code>openrouter/auto</code> as model name to let OpenRouter choose the best model for your prompt.</p><p><strong>Provider Preferences</strong>: Route models through specific providers for better rates or availability.</p><p><strong>Usage Tracking</strong>: Monitor all model usage in one dashboard at <a href="https://openrouter.ai/activity" target="_blank" rel="noreferrer">https://openrouter.ai/activity</a></p><h4 id="configuration-tips-1" tabindex="-1">Configuration Tips <a class="header-anchor" href="#configuration-tips-1" aria-label="Permalink to &quot;Configuration Tips&quot;">‚Äã</a></h4><p><strong>Custom Headers</strong> (Optional but Recommended):</p><p>Add these in Kelivo provider settings:</p><div class="language-json vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">json</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">{</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  &quot;HTTP-Referer&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;https://kelivo.app&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  &quot;X-Title&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Kelivo Mobile&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div><p><strong>Benefits</strong>:</p><ul><li>Helps OpenRouter improve service</li><li>May unlock better rates in future</li><li>Enables better analytics</li></ul><p><strong>Model Selection</strong>:</p><p>Format: <code>provider/model-name</code></p><ul><li>‚úÖ Correct: <code>anthropic/claude-3.5-sonnet</code></li><li>‚úÖ Correct: <code>openai/gpt-4o</code></li><li>‚úÖ Correct: <code>google/gemini-pro-1.5</code></li><li>‚ùå Wrong: <code>claude-3.5-sonnet</code> (missing provider)</li></ul><p><strong>Find Models</strong>: Browse all available models at <a href="https://openrouter.ai/models" target="_blank" rel="noreferrer">https://openrouter.ai/models</a></p><h4 id="troubleshooting-1" tabindex="-1">Troubleshooting <a class="header-anchor" href="#troubleshooting-1" aria-label="Permalink to &quot;Troubleshooting&quot;">‚Äã</a></h4><p><strong>Invalid Model Name</strong>:</p><ul><li>Use full format: <code>provider/model-name</code></li><li>Check model list: <a href="https://openrouter.ai/models" target="_blank" rel="noreferrer">https://openrouter.ai/models</a></li><li>Some models require special access</li></ul><p><strong>Insufficient Credits</strong>:</p><ul><li>Add more credits at <a href="https://openrouter.ai/credits" target="_blank" rel="noreferrer">https://openrouter.ai/credits</a></li><li>Minimum $5 top-up</li><li>Credits never expire</li></ul><p><strong>Rate Limits</strong>:</p><ul><li>Most models have no rate limits</li><li>Some premium models may have provider-specific limits</li><li>Check model page for details</li></ul><p><strong>Model Unavailable</strong>:</p><ul><li>Provider may be temporarily down</li><li>Use model fallbacks feature</li><li>Try alternative model</li></ul><h4 id="cost-optimization" tabindex="-1">Cost Optimization <a class="header-anchor" href="#cost-optimization" aria-label="Permalink to &quot;Cost Optimization&quot;">‚Äã</a></h4><p><strong>Smart Model Selection</strong>:</p><ol><li><strong>Start with free tier</strong>: google/gemini-pro-1.5 for most tasks</li><li><strong>Use cheap models</strong>: mistralai/mistral-7b-instruct for simple tasks</li><li><strong>Reserve premium</strong>: anthropic/claude-3.5-sonnet for complex work</li><li><strong>Avoid expensive</strong>: Only use o1-preview or claude-3-opus when needed</li></ol><p><strong>Monitor Usage</strong>:</p><ul><li>Check dashboard: <a href="https://openrouter.ai/activity" target="_blank" rel="noreferrer">https://openrouter.ai/activity</a></li><li>Set up spending alerts</li><li>Review per-model costs</li></ul><p><strong>Context Management</strong>:</p><ul><li>Keep conversations focused</li><li>Clear history regularly</li><li>Use smaller models for follow-ups</li></ul><h4 id="resources" tabindex="-1">Resources <a class="header-anchor" href="#resources" aria-label="Permalink to &quot;Resources&quot;">‚Äã</a></h4><p><strong>Official Links</strong>:</p><ul><li>OpenRouter Homepage: <a href="https://openrouter.ai" target="_blank" rel="noreferrer">https://openrouter.ai</a></li><li>Model List: <a href="https://openrouter.ai/models" target="_blank" rel="noreferrer">https://openrouter.ai/models</a></li><li>Pricing: <a href="https://openrouter.ai/docs#pricing" target="_blank" rel="noreferrer">https://openrouter.ai/docs#pricing</a></li><li>API Docs: <a href="https://openrouter.ai/docs" target="_blank" rel="noreferrer">https://openrouter.ai/docs</a></li><li>Activity Dashboard: <a href="https://openrouter.ai/activity" target="_blank" rel="noreferrer">https://openrouter.ai/activity</a></li></ul><p><strong>Community</strong>:</p><ul><li>Discord: <a href="https://discord.gg/openrouter" target="_blank" rel="noreferrer">https://discord.gg/openrouter</a></li><li>GitHub: <a href="https://github.com/OpenRouterTeam" target="_blank" rel="noreferrer">https://github.com/OpenRouterTeam</a></li></ul><h3 id="other-openai-compatible-providers" tabindex="-1">Other OpenAI-Compatible Providers <a class="header-anchor" href="#other-openai-compatible-providers" aria-label="Permalink to &quot;Other OpenAI-Compatible Providers&quot;">‚Äã</a></h3><p><strong>Examples</strong>:</p><ul><li><strong>Together AI</strong>: Open-source models, fast inference</li><li><strong>Groq</strong>: Ultra-fast inference for open models</li><li><strong>Anyscale</strong>: Llama and Mistral models</li><li><strong>Fireworks AI</strong>: Fast open-source models</li></ul><p><strong>Setup</strong>: Change base URL, use their API key</p><h2 id="best-practices" tabindex="-1">Best Practices <a class="header-anchor" href="#best-practices" aria-label="Permalink to &quot;Best Practices&quot;">‚Äã</a></h2><h3 id="model-selection" tabindex="-1">Model Selection <a class="header-anchor" href="#model-selection" aria-label="Permalink to &quot;Model Selection&quot;">‚Äã</a></h3><ul><li><strong>Default</strong>: gpt-4o for most users</li><li><strong>Budget</strong>: gpt-3.5-turbo for high volume</li><li><strong>Complex tasks</strong>: gpt-4-turbo or o1-preview</li><li><strong>Code</strong>: o1-mini or gpt-4o</li><li><strong>Speed</strong>: gpt-3.5-turbo or gpt-4o</li></ul><h3 id="cost-optimization-1" tabindex="-1">Cost Optimization <a class="header-anchor" href="#cost-optimization-1" aria-label="Permalink to &quot;Cost Optimization&quot;">‚Äã</a></h3><ol><li><strong>Right-size model</strong>: Don&#39;t use GPT-4 for simple tasks</li><li><strong>Shorten prompts</strong>: Remove unnecessary context</li><li><strong>Manage history</strong>: Limit conversation length</li><li><strong>Use caching</strong>: Reuse system prompts (provider-level)</li><li><strong>Monitor usage</strong>: Check OpenAI dashboard regularly</li></ol><h3 id="security" tabindex="-1">Security <a class="header-anchor" href="#security" aria-label="Permalink to &quot;Security&quot;">‚Äã</a></h3><ol><li><strong>Protect API keys</strong>: Never share or commit to repos</li><li><strong>Rotate keys</strong>: Periodic rotation policy</li><li><strong>Use separate keys</strong>: Different keys for dev/prod</li><li><strong>Monitor usage</strong>: Watch for unusual activity</li><li><strong>Set spending limits</strong>: In OpenAI dashboard</li></ol><h3 id="performance" tabindex="-1">Performance <a class="header-anchor" href="#performance" aria-label="Permalink to &quot;Performance&quot;">‚Äã</a></h3><ol><li><strong>Enable streaming</strong>: For better UX</li><li><strong>Parallel requests</strong>: For independent tasks</li><li><strong>Appropriate timeouts</strong>: 60s for GPT-4, 30s for GPT-3.5</li><li><strong>Retry logic</strong>: Handle transient failures</li></ol><h2 id="resources-1" tabindex="-1">Resources <a class="header-anchor" href="#resources-1" aria-label="Permalink to &quot;Resources&quot;">‚Äã</a></h2><h3 id="official-documentation" tabindex="-1">Official Documentation <a class="header-anchor" href="#official-documentation" aria-label="Permalink to &quot;Official Documentation&quot;">‚Äã</a></h3><ul><li>OpenAI Platform: <a href="https://platform.openai.com" target="_blank" rel="noreferrer">https://platform.openai.com</a></li><li>API Reference: <a href="https://platform.openai.com/docs/api-reference" target="_blank" rel="noreferrer">https://platform.openai.com/docs/api-reference</a></li><li>Pricing: <a href="https://openai.com/pricing" target="_blank" rel="noreferrer">https://openai.com/pricing</a></li><li>Status: <a href="https://status.openai.com" target="_blank" rel="noreferrer">https://status.openai.com</a></li></ul><h3 id="community" tabindex="-1">Community <a class="header-anchor" href="#community" aria-label="Permalink to &quot;Community&quot;">‚Äã</a></h3><ul><li>OpenAI Forum: <a href="https://community.openai.com" target="_blank" rel="noreferrer">https://community.openai.com</a></li><li>Discord: Official OpenAI community</li><li>Reddit: r/OpenAI</li></ul><h3 id="learning" tabindex="-1">Learning <a class="header-anchor" href="#learning" aria-label="Permalink to &quot;Learning&quot;">‚Äã</a></h3><ul><li>OpenAI Cookbook: Code examples</li><li>Prompt Engineering Guide: Best practices</li><li>Model documentation: Capabilities and limits</li></ul><h2 id="next-steps" tabindex="-1">Next Steps <a class="header-anchor" href="#next-steps" aria-label="Permalink to &quot;Next Steps&quot;">‚Äã</a></h2><ul><li>Configure your <a href="/docs/assistant/basics">Assistant</a> with OpenAI</li><li>Write effective <a href="/docs/assistant/prompts">Prompts</a> for GPT models</li><li>Explore <a href="/docs/assistant/custom-requests">Custom Requests</a> for advanced setups</li><li>Compare with other providers: <a href="/docs/providers/anthropic">Anthropic</a>, <a href="/docs/providers/google">Google</a></li></ul><p>Need help? Check the <a href="/docs/getting-started/faq">FAQ</a> or <a href="https://github.com/Chevey339/kelivo/issues" target="_blank" rel="noreferrer">open an issue</a>.</p>`,234)])])}const c=t(r,[["render",a]]);export{h as __pageData,c as default};
