import{_ as t,c as o,o as i,aM as a}from"./chunks/framework.IHAHILrl.js";const m=JSON.parse('{"title":"Terminology","description":"","frontmatter":{"title":"Terminology","date":"2025-09-02T01:16:27.000Z"},"headers":[],"relativePath":"docs/getting-started/terminology.md","filePath":"docs/getting-started/terminology.md","lastUpdated":1763177234000}'),s={name:"docs/getting-started/terminology.md"};function r(l,e,n,p,d,c){return i(),o("div",null,[...e[0]||(e[0]=[a(`<h1 id="terminology" tabindex="-1">Terminology <a class="header-anchor" href="#terminology" aria-label="Permalink to &quot;Terminology&quot;">​</a></h1><p>Understanding key AI and Kelivo concepts will help you get the most from the app.</p><h2 id="core-ai-concepts" tabindex="-1">Core AI Concepts <a class="header-anchor" href="#core-ai-concepts" aria-label="Permalink to &quot;Core AI Concepts&quot;">​</a></h2><h3 id="model" tabindex="-1">Model <a class="header-anchor" href="#model" aria-label="Permalink to &quot;Model&quot;">​</a></h3><p>The underlying AI system that processes your messages and generates responses. Examples:</p><ul><li><strong>OpenAI</strong>: gpt-4o, gpt-4-turbo, gpt-3.5-turbo, o1-preview</li><li><strong>Google</strong>: gemini-1.5-pro, gemini-1.5-flash, gemini-2.0-flash</li><li><strong>Anthropic</strong>: claude-3.5-sonnet, claude-3-opus, claude-3-haiku</li></ul><p>Each model has different capabilities, speeds, costs, and specializations.</p><h3 id="context-window" tabindex="-1">Context Window <a class="header-anchor" href="#context-window" aria-label="Permalink to &quot;Context Window&quot;">​</a></h3><p>The maximum amount of text (measured in tokens) that a model can process at once, including both your prompt and the model&#39;s response.</p><p>Examples:</p><ul><li>GPT-4o: 128,000 tokens (~96,000 words)</li><li>Claude 3.5 Sonnet: 200,000 tokens (~150,000 words)</li><li>Gemini 1.5 Pro: 2,000,000 tokens (~1.5 million words)</li></ul><p>When a conversation exceeds the context window, older messages may be dropped or summarized.</p><h3 id="token" tabindex="-1">Token <a class="header-anchor" href="#token" aria-label="Permalink to &quot;Token&quot;">​</a></h3><p>A sub-word unit used for counting text in AI models. Tokens can be words, parts of words, or punctuation.</p><p><strong>Rough estimates</strong>:</p><ul><li>1 token ≈ 0.75 words in English</li><li>100 tokens ≈ 75 words</li><li>1,000 tokens ≈ 750 words</li></ul><p>Tokens are used for:</p><ul><li><strong>Billing</strong>: Providers charge per token (input + output)</li><li><strong>Context limits</strong>: Determines how much conversation fits in memory</li><li><strong>Rate limits</strong>: Some providers limit tokens per minute</li></ul><p><strong>Tip</strong>: Use shorter messages and clear conversation history to save tokens.</p><h3 id="temperature" tabindex="-1">Temperature <a class="header-anchor" href="#temperature" aria-label="Permalink to &quot;Temperature&quot;">​</a></h3><p>A parameter (0.0 to 2.0) that controls response randomness and creativity.</p><ul><li><strong>0.0 - 0.3</strong>: Focused, deterministic, factual (best for coding, math, analysis)</li><li><strong>0.4 - 0.7</strong>: Balanced creativity and coherence (general conversation)</li><li><strong>0.8 - 1.2</strong>: Creative, varied responses (brainstorming, storytelling)</li><li><strong>1.3 - 2.0</strong>: Highly random, experimental (creative writing, exploration)</li></ul><p><strong>Default recommendation</strong>: 0.7 for most use cases.</p><h3 id="top-p-nucleus-sampling" tabindex="-1">Top-p (Nucleus Sampling) <a class="header-anchor" href="#top-p-nucleus-sampling" aria-label="Permalink to &quot;Top-p (Nucleus Sampling)&quot;">​</a></h3><p>A sampling method (0.0 to 1.0) that controls output diversity by considering only the most likely tokens.</p><ul><li><strong>0.1 - 0.5</strong>: More focused, consistent responses</li><li><strong>0.6 - 0.9</strong>: Balanced diversity (recommended)</li><li><strong>0.95 - 1.0</strong>: Maximum diversity, more unpredictable</li></ul><p><strong>Common setting</strong>: 0.9. Usually used together with temperature.</p><h3 id="top-k" tabindex="-1">Top-k <a class="header-anchor" href="#top-k" aria-label="Permalink to &quot;Top-k&quot;">​</a></h3><p>Limits the model to choose from only the top K most likely next tokens.</p><ul><li><strong>Lower values (1-10)</strong>: More focused output</li><li><strong>Higher values (40-100)</strong>: More diverse output</li></ul><p>Not all models or providers support top-k. Top-p is more commonly used.</p><h2 id="kelivo-specific-terms" tabindex="-1">Kelivo-Specific Terms <a class="header-anchor" href="#kelivo-specific-terms" aria-label="Permalink to &quot;Kelivo-Specific Terms&quot;">​</a></h2><h3 id="provider" tabindex="-1">Provider <a class="header-anchor" href="#provider" aria-label="Permalink to &quot;Provider&quot;">​</a></h3><p>An AI service that offers API access to models. Kelivo supports:</p><ul><li><strong>OpenAI</strong>: ChatGPT models (GPT-4, GPT-3.5, o1, etc.)</li><li><strong>Anthropic</strong>: Claude models</li><li><strong>Google</strong>: Gemini models</li><li><strong>Zhipu</strong>: GLM models (Chinese provider)</li><li><strong>Custom</strong>: Self-hosted or compatible APIs</li></ul><p>Each provider requires its own API key and has different pricing, features, and rate limits.</p><h3 id="assistant" tabindex="-1">Assistant <a class="header-anchor" href="#assistant" aria-label="Permalink to &quot;Assistant&quot;">​</a></h3><p>A reusable configuration in Kelivo that bundles:</p><ul><li>Default provider and model</li><li>System prompt (personality/instructions)</li><li>Temperature and sampling parameters</li><li>Memory settings</li><li>Tool access (TTS, search, MCP)</li></ul><p><strong>Benefits</strong>:</p><ul><li>Quickly switch between different AI personas</li><li>Maintain consistent behavior across chats</li><li>Share configurations (export/import)</li></ul><p><strong>Examples</strong>:</p><ul><li>&quot;Python Tutor&quot; - GPT-4o with coding-focused prompt</li><li>&quot;Creative Writer&quot; - Claude 3.5 Sonnet with high temperature</li><li>&quot;Research Assistant&quot; - Gemini Pro with web search enabled</li></ul><h3 id="system-prompt" tabindex="-1">System Prompt <a class="header-anchor" href="#system-prompt" aria-label="Permalink to &quot;System Prompt&quot;">​</a></h3><p>Instructions that define an assistant&#39;s role, behavior, and constraints. The system prompt is sent with every message but isn&#39;t visible in the chat.</p><p><strong>Example</strong>:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>You are an expert Python developer. Provide clean, well-commented code </span></span>
<span class="line"><span>with explanations. Focus on best practices and efficient solutions.</span></span></code></pre></div><p>System prompts help:</p><ul><li>Set personality and tone</li><li>Define expertise areas</li><li>Establish rules and limitations</li><li>Provide context or background</li></ul><p>See <a href="/docs/assistant/prompts">Prompts</a> for best practices.</p><h3 id="memory" tabindex="-1">Memory <a class="header-anchor" href="#memory" aria-label="Permalink to &quot;Memory&quot;">​</a></h3><p>A feature that allows assistants to remember key information across conversations.</p><p><strong>How it works</strong>:</p><ol><li>Important facts are extracted from conversations</li><li>Summaries are stored locally on your device</li><li>Relevant memories are included in future prompts</li></ol><p><strong>Use cases</strong>:</p><ul><li>Personal AI that remembers your preferences</li><li>Project assistant that recalls previous discussions</li><li>Long-term research companion</li></ul><p><strong>Privacy</strong>: All memories are stored locally and never uploaded.</p><h2 id="advanced-features" tabindex="-1">Advanced Features <a class="header-anchor" href="#advanced-features" aria-label="Permalink to &quot;Advanced Features&quot;">​</a></h2><h3 id="tool-calling-function-calling" tabindex="-1">Tool Calling (Function Calling) <a class="header-anchor" href="#tool-calling-function-calling" aria-label="Permalink to &quot;Tool Calling (Function Calling)&quot;">​</a></h3><p>The ability for AI models to request structured actions like:</p><ul><li>Searching the web</li><li>Performing calculations</li><li>Querying databases</li><li>Calling external APIs</li></ul><p>Kelivo supports tool calling through:</p><ul><li>Built-in web search</li><li>MCP (Model Context Protocol) integrations</li></ul><h3 id="mcp-model-context-protocol" tabindex="-1">MCP (Model Context Protocol) <a class="header-anchor" href="#mcp-model-context-protocol" aria-label="Permalink to &quot;MCP (Model Context Protocol)&quot;">​</a></h3><p>An open standard for connecting AI models to external tools and data sources.</p><p><strong>MCP enables</strong>:</p><ul><li>File system access</li><li>Database queries</li><li>Git operations</li><li>Custom tool integrations</li></ul><p>See <a href="/docs/assistant/mcp">MCP documentation</a> for setup details.</p><h3 id="api-key" tabindex="-1">API Key <a class="header-anchor" href="#api-key" aria-label="Permalink to &quot;API Key&quot;">​</a></h3><p>A secret credential that authenticates your requests to an AI provider.</p><p><strong>Important</strong>:</p><ul><li>Keep API keys private and secure</li><li>Never share keys in screenshots or public forums</li><li>Revoke compromised keys immediately</li><li>Monitor usage to prevent unexpected charges</li></ul><p>In Kelivo, API keys are stored in encrypted device storage.</p><h3 id="base-url" tabindex="-1">Base URL <a class="header-anchor" href="#base-url" aria-label="Permalink to &quot;Base URL&quot;">​</a></h3><p>The endpoint URL for API requests. You can customize this to:</p><ul><li>Use API gateways or proxies</li><li>Connect to self-hosted models</li><li>Route through corporate networks</li></ul><p><strong>Examples</strong>:</p><ul><li>OpenAI default: <code>https://api.openai.com/v1</code></li><li>Custom proxy: <code>https://my-gateway.com/v1</code></li><li>Local model: <code>http://localhost:8000</code></li></ul><h3 id="embeddings" tabindex="-1">Embeddings <a class="header-anchor" href="#embeddings" aria-label="Permalink to &quot;Embeddings&quot;">​</a></h3><p>Vector representations of text used for semantic search and similarity matching.</p><p><strong>Use cases</strong>:</p><ul><li>Document search</li><li>Context retrieval for Memory</li><li>Semantic similarity</li></ul><p>Kelivo may use embeddings internally for memory and search features.</p><h3 id="tts-text-to-speech" tabindex="-1">TTS (Text-to-Speech) <a class="header-anchor" href="#tts-text-to-speech" aria-label="Permalink to &quot;TTS (Text-to-Speech)&quot;">​</a></h3><p>Technology that converts written text to spoken audio.</p><p><strong>In Kelivo</strong>:</p><ul><li>Tap the speaker icon on any message</li><li>Customize voice, speed, and pitch</li><li>Supports multiple languages</li></ul><p>Useful for:</p><ul><li>Accessibility</li><li>Hands-free interaction</li><li>Language learning</li></ul><h3 id="vision-multimodal" tabindex="-1">Vision (Multimodal) <a class="header-anchor" href="#vision-multimodal" aria-label="Permalink to &quot;Vision (Multimodal)&quot;">​</a></h3><p>The ability of AI models to process and understand images in addition to text.</p><p><strong>Vision-capable models</strong>:</p><ul><li>GPT-4o, GPT-4 Turbo (OpenAI)</li><li>Claude 3.5 Sonnet, Claude 3 Opus (Anthropic)</li><li>Gemini 1.5 Pro/Flash (Google)</li></ul><p><strong>Use cases</strong>:</p><ul><li>Describe images</li><li>Extract text from screenshots (OCR)</li><li>Analyze diagrams or charts</li><li>Answer questions about photos</li></ul><h3 id="rate-limit" tabindex="-1">Rate Limit <a class="header-anchor" href="#rate-limit" aria-label="Permalink to &quot;Rate Limit&quot;">​</a></h3><p>Restrictions on how many requests you can make to an API within a time period.</p><p><strong>Common limits</strong>:</p><ul><li>Requests per minute (RPM)</li><li>Tokens per minute (TPM)</li><li>Requests per day (RPD)</li></ul><p><strong>Example</strong>: OpenAI free tier might allow 3 RPM and 40,000 TPM.</p><p><strong>Tip</strong>: Upgrade to paid tiers for higher limits, or wait between requests.</p><h3 id="streaming" tabindex="-1">Streaming <a class="header-anchor" href="#streaming" aria-label="Permalink to &quot;Streaming&quot;">​</a></h3><p>Receiving AI responses in real-time as they&#39;re generated, rather than waiting for the complete response.</p><p><strong>Benefits</strong>:</p><ul><li>Faster perceived response time</li><li>Better user experience for long outputs</li><li>Can stop generation mid-response</li></ul><p>Kelivo supports streaming for compatible providers.</p><h2 id="model-specific-terms" tabindex="-1">Model-Specific Terms <a class="header-anchor" href="#model-specific-terms" aria-label="Permalink to &quot;Model-Specific Terms&quot;">​</a></h2><h3 id="fine-tuning" tabindex="-1">Fine-tuning <a class="header-anchor" href="#fine-tuning" aria-label="Permalink to &quot;Fine-tuning&quot;">​</a></h3><p>Training a model on custom data to specialize its behavior.</p><p><strong>Note</strong>: Kelivo doesn&#39;t support fine-tuning directly. Use provider platforms (OpenAI, etc.) for this.</p><h3 id="reasoning-models-o1-o3" tabindex="-1">Reasoning Models (o1, o3) <a class="header-anchor" href="#reasoning-models-o1-o3" aria-label="Permalink to &quot;Reasoning Models (o1, o3)&quot;">​</a></h3><p>Advanced models that think through problems step-by-step before answering.</p><p><strong>Characteristics</strong>:</p><ul><li>Better at complex reasoning, math, and coding</li><li>Longer processing time</li><li>Different pricing structure</li><li>May not support all features (vision, streaming)</li></ul><p><strong>Examples</strong>: o1-preview, o1-mini (OpenAI)</p><h3 id="flash-models" tabindex="-1">Flash Models <a class="header-anchor" href="#flash-models" aria-label="Permalink to &quot;Flash Models&quot;">​</a></h3><p>Optimized for speed and cost-efficiency, with slightly lower capability than flagship models.</p><p><strong>Examples</strong>:</p><ul><li>gemini-1.5-flash (Google)</li><li>claude-3-haiku (Anthropic)</li></ul><p><strong>Use cases</strong>: Quick tasks, high-volume usage, cost-sensitive applications.</p><h2 id="need-more-help" tabindex="-1">Need More Help? <a class="header-anchor" href="#need-more-help" aria-label="Permalink to &quot;Need More Help?&quot;">​</a></h2><ul><li>Check the <a href="/docs/getting-started/faq">FAQ</a> for common questions</li><li>Read <a href="/docs/assistant/basics">Assistant documentation</a> for configuration details</li><li>Explore <a href="/docs/providers/openai">Provider guides</a> for specific setup instructions</li></ul>`,122)])])}const h=t(s,[["render",r]]);export{m as __pageData,h as default};
